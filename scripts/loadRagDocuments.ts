/**
 * RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
 *
 * ä½¿ç”¨æ–¹æ³•:
 *   npx ts-node scripts/loadRagDocuments.ts [ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹]
 *
 * ä¾‹:
 *   npx ts-node scripts/loadRagDocuments.ts ./knowledge
 *
 * ç’°å¢ƒå¤‰æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰:
 *   OLLAMA_URL - Ollamaã®URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:11434ï¼‰
 *   OLLAMA_EMBEDDING_MODEL - Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: nomic-embed-textï¼‰
 *   CHROMA_URL - ChromaDBã®URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:8000ï¼‰
 *   RAG_COLLECTION_NAME - ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: aituber_knowledgeï¼‰
 */

import * as path from 'path'
import * as dotenv from 'dotenv'

// .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
dotenv.config()

// LangChainã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import { TextLoader } from 'langchain/document_loaders/fs/text'
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter'
import { Document } from 'langchain/document'
import { OllamaEmbeddings } from '@langchain/community/embeddings/ollama'
import { Chroma } from '@langchain/community/vectorstores/chroma'
import * as fs from 'fs'

// è¨­å®š
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434'
const EMBEDDING_MODEL =
  process.env.OLLAMA_EMBEDDING_MODEL ||
  process.env.NEXT_PUBLIC_RAG_EMBEDDING_MODEL ||
  'nomic-embed-text'
const CHROMA_URL =
  process.env.CHROMA_URL ||
  process.env.NEXT_PUBLIC_RAG_CHROMA_URL ||
  'http://localhost:8000'
const COLLECTION_NAME =
  process.env.RAG_COLLECTION_NAME ||
  process.env.NEXT_PUBLIC_RAG_COLLECTION_NAME ||
  'aituber_knowledge'

/**
 * ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹
 */
async function loadAndSplitDocument(
  filePath: string,
  chunkSize: number = 1000,
  chunkOverlap: number = 200
): Promise<Document[]> {
  const loader = new TextLoader(filePath)
  const docs = await loader.load()

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize,
    chunkOverlap,
    separators: ['\n\n', '\n', ' ', ''],
  })

  return await splitter.splitDocuments(docs)
}

/**
 * ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
 */
async function loadDocumentsFromDirectory(
  dirPath: string,
  extensions: string[] = ['.txt', '.md']
): Promise<Document[]> {
  const allDocs: Document[] = []

  if (!fs.existsSync(dirPath)) {
    throw new Error(`Directory not found: ${dirPath}`)
  }

  const files = fs.readdirSync(dirPath)

  for (const file of files) {
    const ext = path.extname(file).toLowerCase()
    if (extensions.includes(ext)) {
      const filePath = path.join(dirPath, file)
      console.log(`ğŸ“„ Loading: ${filePath}`)
      const docs = await loadAndSplitDocument(filePath)
      allDocs.push(...docs)
    }
  }

  return allDocs
}

/**
 * ãƒ¡ã‚¤ãƒ³å‡¦ç†
 */
async function main() {
  const args = process.argv.slice(2)
  const knowledgeDir = args[0] || path.join(process.cwd(), 'knowledge')

  console.log('ğŸš€ RAG Document Loader')
  console.log('========================')
  console.log(`ğŸ“ Knowledge directory: ${knowledgeDir}`)
  console.log(`ğŸ”— Ollama URL: ${OLLAMA_URL}`)
  console.log(`ğŸ§  Embedding model: ${EMBEDDING_MODEL}`)
  console.log(`ğŸ’¾ ChromaDB URL: ${CHROMA_URL}`)
  console.log(`ğŸ“š Collection name: ${COLLECTION_NAME}`)
  console.log('========================\n')

  // ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨ç¢ºèª
  if (!fs.existsSync(knowledgeDir)) {
    console.error(`âŒ Error: Directory not found: ${knowledgeDir}`)
    console.log(
      '\nğŸ’¡ Hint: Create the directory and add .txt or .md files to it.'
    )
    process.exit(1)
  }

  // ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
  console.log('ğŸ“š Loading documents...')
  const documents = await loadDocumentsFromDirectory(knowledgeDir)

  if (documents.length === 0) {
    console.error('âŒ Error: No .txt or .md files found in the directory')
    console.log('\nğŸ’¡ Hint: Add some .txt or .md files to the knowledge directory.')
    process.exit(1)
  }

  console.log(`âœ… Loaded ${documents.length} chunks from documents\n`)

  // Embeddingä½œæˆ
  console.log('ğŸ”„ Creating embeddings...')
  const embeddings = new OllamaEmbeddings({
    baseUrl: OLLAMA_URL,
    model: EMBEDDING_MODEL,
  })

  // ChromaDBã«ä¿å­˜
  console.log('ğŸ’¾ Storing in ChromaDB...')
  try {
    await Chroma.fromDocuments(documents, embeddings, {
      collectionName: COLLECTION_NAME,
      url: CHROMA_URL,
    })
    console.log('\nâœ… Documents loaded successfully!')
    console.log(`ğŸ“Š Total chunks: ${documents.length}`)
    console.log(`ğŸ“š Collection: ${COLLECTION_NAME}`)
  } catch (error) {
    console.error('\nâŒ Error storing documents:', error)
    console.log('\nğŸ’¡ Troubleshooting:')
    console.log('  1. Make sure ChromaDB is running: docker run -d -p 8000:8000 chromadb/chroma')
    console.log('  2. Make sure Ollama is running with the embedding model: ollama pull ' + EMBEDDING_MODEL)
    process.exit(1)
  }
}

main().catch((error) => {
  console.error('Fatal error:', error)
  process.exit(1)
})
